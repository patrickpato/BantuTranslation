{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKKD2bzWRf31",
        "outputId": "36c28f21-bf22-4cb6-ece1-6b89070bb6d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUiZSmd-nw8S",
        "outputId": "92b3b116-dd9d-4296-a0c7-e9ab3056f7be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package python-pil\n",
            "E: Unable to locate package python-lxml\n"
          ]
        }
      ],
      "source": [
        "!apt-get -y install protobuf-compiler python-pil python-lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLEFcu7tn5du",
        "outputId": "d4583857-ffc8-4334-f461-cb709484ada4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openNMT-py\n",
            "  Downloading OpenNMT_py-3.4-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.5/250.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<2.1,>=1.13 in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (2.0.1+cu118)\n",
            "Collecting configargparse (from openNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting ctranslate2<4,>=3.2 (from openNMT-py)\n",
            "  Downloading ctranslate2-3.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (2.13.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (2.2.5)\n",
            "Collecting waitress (from openNMT-py)\n",
            "  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.35 (from openNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (6.0.1)\n",
            "Collecting sacrebleu (from openNMT-py)\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from openNMT-py)\n",
            "  Downloading rapidfuzz-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from openNMT-py)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel (from openNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (3.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.2->openNMT-py) (1.23.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (3.4.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (0.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->openNMT-py) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->openNMT-py) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->openNMT-py) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->openNMT-py) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->openNMT-py) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->openNMT-py) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=1.13->openNMT-py) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=1.13->openNMT-py) (16.0.6)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel->openNMT-py)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->openNMT-py) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->openNMT-py) (8.1.7)\n",
            "Collecting portalocker (from sacrebleu->openNMT-py)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->openNMT-py) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->openNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->openNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->openNMT-py) (4.9.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (4.66.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (1.10.12)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->openNMT-py) (3.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->openNMT-py) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->openNMT-py) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->openNMT-py) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->openNMT-py) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->openNMT-py) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.1,>=1.13->openNMT-py) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->openNMT-py) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->openNMT-py) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->openNMT-py) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->openNMT-py) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->openNMT-py) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->openNMT-py) (0.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.1,>=1.13->openNMT-py) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->openNMT-py) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->openNMT-py) (3.2.2)\n",
            "Installing collected packages: waitress, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, ctranslate2, configargparse, colorama, sacrebleu, fasttext-wheel, openNMT-py\n",
            "Successfully installed colorama-0.4.6 configargparse-1.7 ctranslate2-3.20.0 fasttext-wheel-0.9.2 openNMT-py-3.4 portalocker-2.8.2 pyahocorasick-2.0.0 pybind11-2.11.1 pyonmttok-1.37.1 rapidfuzz-3.3.0 sacrebleu-2.3.1 waitress-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openNMT-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3BPgqwFl9PH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-sDfaoRoKC8"
      },
      "outputs": [],
      "source": [
        "config = '''# config.yaml\n",
        "\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# Training files\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: /content/drive/MyDrive/EngSwaBBC/cleanGoURMETEng.txt.subword.train\n",
        "        path_tgt: /content/drive/MyDrive/EngSwaBBC/cleanGoURMETSwa.txt.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: /content/drive/MyDrive/EngSwaBBC/cleanGoURMETEng.txt.subword.dev\n",
        "        path_tgt: /content/drive/MyDrive/EngSwaBBC/cleanGoURMETSwa.txt.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabulary files, generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# Vocabulary size - should be the same as in sentence piece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 64\n",
        "tgt_seq_length: 64\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: /content/drive/MyDrive/EngSwaBBC/source.model\n",
        "tgt_subword_model: /content/drive/MyDrive/EngSwaBBC/target.model\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model.engswa\n",
        "\n",
        "# Stop training if it does not imporve after n validations\n",
        "early_stopping: 10\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 2023\n",
        "\n",
        "# Default: 100000 - Train the model to max n steps\n",
        "# Increase to 200000 or more for large datasets\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 15000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 4000\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 5000\n",
        "report_every: 100\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 1024  # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 1024\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 0.4\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 16\n",
        "dec_layers: 16\n",
        "heads: 16\n",
        "hidden_size: 1024\n",
        "word_vec_size: 1024\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lNRjFUyFMl9",
        "outputId": "032c4bc2-fe36-4e12-ec62-fed02c0a1e44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-09-22 01:41:09.135623: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-22 01:41:12.360778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-09-22 01:41:14.603021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-22 01:41:14.603471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-22 01:41:14.603641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2023-09-22 01:41:16,306 INFO] Counter vocab from -1 samples.\n",
            "[2023-09-22 01:41:16,306 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2023-09-22 01:41:23,808 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2744)\n",
            "\n",
            "[2023-09-22 01:41:23,959 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2769)\n",
            "\n",
            "[2023-09-22 01:41:24,135 INFO] Counters src: 45474\n",
            "[2023-09-22 01:41:24,135 INFO] Counters tgt: 46914\n"
          ]
        }
      ],
      "source": [
        "!onmt_build_vocab -config /content/config.yaml -n_sample -1 -num_threads 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFY_xrILFTPL",
        "outputId": "cedf1a3e-353a-4e86-abe9-9c9de294a845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-09-22 01:41:29.446119: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-22 01:41:30.489484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-09-22 01:41:31.952484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-22 01:41:31.952899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-22 01:41:31.953077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2023-09-22 01:41:32,693 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2023-09-22 01:41:32,693 INFO] Parsed 2 corpora from -data.\n",
            "[2023-09-22 01:41:32,694 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2023-09-22 01:41:32,999 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', ',', '▁the', '.', '▁of', '▁and', '▁to']\n",
            "[2023-09-22 01:41:32,999 INFO] The decoder start token is: <s>\n",
            "[2023-09-22 01:41:32,999 INFO] Building model...\n",
            "[2023-09-22 01:41:41,201 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2023-09-22 01:41:41,201 INFO] Non quantized layer compute is fp16\n",
            "[2023-09-22 01:41:49,302 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(45480, 1024, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-15): 16 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(46920, 1024, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-15): 16 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=1024, out_features=46920, bias=True)\n",
            ")\n",
            "[2023-09-22 01:41:49,308 INFO] encoder: 180856832\n",
            "[2023-09-22 01:41:49,308 INFO] decoder: 297566024\n",
            "[2023-09-22 01:41:49,308 INFO] * number of parameters: 478422856\n",
            "[2023-09-22 01:41:49,310 INFO] Trainable parameters = {'torch.float32': 478422856, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2023-09-22 01:41:49,310 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2023-09-22 01:41:49,310 INFO]  * src vocab size = 45480\n",
            "[2023-09-22 01:41:49,310 INFO]  * tgt vocab size = 46920\n",
            "[2023-09-22 01:41:49,315 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2023-09-22 01:41:49,315 INFO] Starting training on GPU: [0]\n",
            "[2023-09-22 01:41:49,315 INFO] Start training loop and validate every 4000 steps...\n",
            "[2023-09-22 01:41:49,315 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=64, tgt_seq_length=64))\n",
            "[2023-09-22 01:41:56,952 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2023-09-22 01:42:03,069 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2023-09-22 01:44:01,132 INFO] Step 100/10000; acc: 4.0; ppl: 33084.8; xent: 10.4; lr: 0.00000; sents:   14969; bsz:  944/ 909/37; 2865/2759 tok/s;    132 sec;\n",
            "[2023-09-22 01:45:49,147 INFO] Step 200/10000; acc: 5.8; ppl: 18830.8; xent: 9.8; lr: 0.00001; sents:   14816; bsz:  945/ 908/37; 3498/3363 tok/s;    240 sec;\n",
            "[2023-09-22 01:47:39,004 INFO] Step 300/10000; acc: 7.6; ppl: 10158.1; xent: 9.2; lr: 0.00001; sents:   14316; bsz:  947/ 913/36; 3447/3325 tok/s;    350 sec;\n",
            "[2023-09-22 01:49:28,937 INFO] Step 400/10000; acc: 10.1; ppl: 4647.2; xent: 8.4; lr: 0.00001; sents:   15050; bsz:  944/ 910/38; 3436/3310 tok/s;    460 sec;\n",
            "[2023-09-22 01:51:21,171 INFO] Step 500/10000; acc: 12.0; ppl: 2637.2; xent: 7.9; lr: 0.00002; sents:   15801; bsz:  944/ 910/40; 3364/3243 tok/s;    572 sec;\n",
            "[2023-09-22 01:53:11,178 INFO] Step 600/10000; acc: 11.9; ppl: 2178.8; xent: 7.7; lr: 0.00002; sents:   14876; bsz:  945/ 911/37; 3435/3312 tok/s;    682 sec;\n",
            "[2023-09-22 01:55:01,777 INFO] Step 700/10000; acc: 12.4; ppl: 1974.4; xent: 7.6; lr: 0.00002; sents:   14980; bsz:  946/ 910/37; 3422/3291 tok/s;    792 sec;\n",
            "[2023-09-22 01:56:52,258 INFO] Step 800/10000; acc: 13.7; ppl: 1852.0; xent: 7.5; lr: 0.00003; sents:   15032; bsz:  945/ 912/38; 3422/3304 tok/s;    903 sec;\n",
            "[2023-09-22 01:58:42,715 INFO] Step 900/10000; acc: 15.5; ppl: 1649.7; xent: 7.4; lr: 0.00003; sents:   15149; bsz:  946/ 904/38; 3425/3275 tok/s;   1013 sec;\n",
            "[2023-09-22 02:00:33,416 INFO] Step 1000/10000; acc: 17.0; ppl: 1412.5; xent: 7.3; lr: 0.00004; sents:   15526; bsz:  944/ 910/39; 3410/3288 tok/s;   1124 sec;\n",
            "[2023-09-22 02:00:33,433 INFO] Saving checkpoint models/model.engswa_step_1000.pt\n",
            "[2023-09-22 02:03:01,023 INFO] Step 1100/10000; acc: 17.6; ppl: 1240.5; xent: 7.1; lr: 0.00004; sents:   14906; bsz:  945/ 910/37; 2562/2466 tok/s;   1272 sec;\n",
            "[2023-09-22 02:04:50,678 INFO] Step 1200/10000; acc: 18.4; ppl: 1146.5; xent: 7.0; lr: 0.00004; sents:   14617; bsz:  945/ 910/37; 3446/3320 tok/s;   1381 sec;\n",
            "[2023-09-22 02:05:04,151 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=11034)\n",
            "\n",
            "[2023-09-22 02:05:04,152 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2023-09-22 02:05:12,480 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2023-09-22 02:07:08,701 INFO] Step 1300/10000; acc: 19.5; ppl: 1005.0; xent: 6.9; lr: 0.00005; sents:   15179; bsz:  945/ 910/38; 2738/2638 tok/s;   1519 sec;\n",
            "[2023-09-22 02:08:59,475 INFO] Step 1400/10000; acc: 20.2; ppl: 912.4; xent: 6.8; lr: 0.00005; sents:   15634; bsz:  941/ 908/39; 3400/3278 tok/s;   1630 sec;\n",
            "[2023-09-22 02:10:50,183 INFO] Step 1500/10000; acc: 20.2; ppl: 874.8; xent: 6.8; lr: 0.00005; sents:   14173; bsz:  944/ 912/35; 3412/3294 tok/s;   1741 sec;\n",
            "[2023-09-22 02:12:41,408 INFO] Step 1600/10000; acc: 21.1; ppl: 768.8; xent: 6.6; lr: 0.00006; sents:   14709; bsz:  948/ 912/37; 3409/3281 tok/s;   1852 sec;\n",
            "[2023-09-22 02:14:32,609 INFO] Step 1700/10000; acc: 21.0; ppl: 763.8; xent: 6.6; lr: 0.00006; sents:   14436; bsz:  945/ 912/36; 3399/3281 tok/s;   1963 sec;\n",
            "[2023-09-22 02:16:23,489 INFO] Step 1800/10000; acc: 22.1; ppl: 693.3; xent: 6.5; lr: 0.00006; sents:   15444; bsz:  945/ 909/39; 3409/3280 tok/s;   2074 sec;\n",
            "[2023-09-22 02:18:15,392 INFO] Step 1900/10000; acc: 22.4; ppl: 647.4; xent: 6.5; lr: 0.00007; sents:   14467; bsz:  948/ 910/36; 3388/3253 tok/s;   2186 sec;\n",
            "[2023-09-22 02:20:08,281 INFO] Step 2000/10000; acc: 22.8; ppl: 616.1; xent: 6.4; lr: 0.00007; sents:   14744; bsz:  946/ 911/37; 3352/3229 tok/s;   2299 sec;\n",
            "[2023-09-22 02:20:08,299 INFO] Saving checkpoint models/model.engswa_step_2000.pt\n",
            "[2023-09-22 02:23:33,873 INFO] Step 2100/10000; acc: 23.2; ppl: 568.9; xent: 6.3; lr: 0.00007; sents:   14452; bsz:  945/ 906/36; 1839/1763 tok/s;   2505 sec;\n",
            "[2023-09-22 02:25:26,293 INFO] Step 2200/10000; acc: 23.5; ppl: 556.8; xent: 6.3; lr: 0.00008; sents:   15389; bsz:  944/ 909/38; 3358/3233 tok/s;   2617 sec;\n",
            "[2023-09-22 02:27:18,352 INFO] Step 2300/10000; acc: 24.0; ppl: 523.7; xent: 6.3; lr: 0.00008; sents:   15984; bsz:  943/ 907/40; 3368/3239 tok/s;   2729 sec;\n",
            "[2023-09-22 02:29:10,342 INFO] Step 2400/10000; acc: 24.9; ppl: 466.8; xent: 6.1; lr: 0.00008; sents:   15552; bsz:  945/ 912/39; 3377/3256 tok/s;   2841 sec;\n",
            "[2023-09-22 02:29:31,860 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=11030)\n",
            "\n",
            "[2023-09-22 02:29:31,860 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2023-09-22 02:29:39,226 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2023-09-22 02:31:36,261 INFO] Step 2500/10000; acc: 25.0; ppl: 446.5; xent: 6.1; lr: 0.00009; sents:   14940; bsz:  946/ 908/37; 2592/2490 tok/s;   2987 sec;\n",
            "[2023-09-22 02:33:28,526 INFO] Step 2600/10000; acc: 25.9; ppl: 417.1; xent: 6.0; lr: 0.00009; sents:   15420; bsz:  943/ 909/39; 3361/3239 tok/s;   3099 sec;\n",
            "[2023-09-22 02:35:20,738 INFO] Step 2700/10000; acc: 26.4; ppl: 385.9; xent: 6.0; lr: 0.00010; sents:   15005; bsz:  945/ 912/38; 3368/3251 tok/s;   3211 sec;\n",
            "[2023-09-22 02:37:12,999 INFO] Step 2800/10000; acc: 27.5; ppl: 358.1; xent: 5.9; lr: 0.00010; sents:   15073; bsz:  945/ 911/38; 3366/3245 tok/s;   3324 sec;\n",
            "[2023-09-22 02:39:05,352 INFO] Step 2900/10000; acc: 27.9; ppl: 349.7; xent: 5.9; lr: 0.00010; sents:   15115; bsz:  945/ 909/38; 3363/3235 tok/s;   3436 sec;\n",
            "[2023-09-22 02:40:57,768 INFO] Step 3000/10000; acc: 29.1; ppl: 315.2; xent: 5.8; lr: 0.00011; sents:   15386; bsz:  945/ 909/38; 3362/3234 tok/s;   3548 sec;\n",
            "[2023-09-22 02:40:57,789 INFO] Saving checkpoint models/model.engswa_step_3000.pt\n",
            "[2023-09-22 02:44:09,624 INFO] Step 3100/10000; acc: 29.6; ppl: 302.8; xent: 5.7; lr: 0.00011; sents:   14961; bsz:  947/ 909/37; 1974/1895 tok/s;   3740 sec;\n",
            "[2023-09-22 02:46:02,152 INFO] Step 3200/10000; acc: 29.3; ppl: 303.5; xent: 5.7; lr: 0.00011; sents:   14276; bsz:  945/ 910/36; 3360/3236 tok/s;   3853 sec;\n",
            "[2023-09-22 02:47:54,599 INFO] Step 3300/10000; acc: 30.7; ppl: 275.0; xent: 5.6; lr: 0.00012; sents:   14836; bsz:  945/ 909/37; 3361/3232 tok/s;   3965 sec;\n",
            "[2023-09-22 02:49:47,013 INFO] Step 3400/10000; acc: 32.1; ppl: 249.1; xent: 5.5; lr: 0.00012; sents:   14503; bsz:  945/ 910/36; 3364/3238 tok/s;   4078 sec;\n",
            "[2023-09-22 02:51:39,180 INFO] Step 3500/10000; acc: 32.9; ppl: 231.8; xent: 5.4; lr: 0.00012; sents:   14904; bsz:  946/ 913/37; 3372/3255 tok/s;   4190 sec;\n",
            "[2023-09-22 02:53:31,003 INFO] Step 3600/10000; acc: 34.4; ppl: 204.6; xent: 5.3; lr: 0.00013; sents:   15415; bsz:  944/ 908/39; 3378/3249 tok/s;   4302 sec;\n",
            "[2023-09-22 02:53:54,381 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=11027)\n",
            "\n",
            "[2023-09-22 02:53:54,382 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2023-09-22 02:54:02,462 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2023-09-22 02:55:49,453 INFO] Step 3700/10000; acc: 36.6; ppl: 166.0; xent: 5.1; lr: 0.00013; sents:   15275; bsz:  945/ 910/38; 2729/2628 tok/s;   4440 sec;\n",
            "[2023-09-22 02:57:41,631 INFO] Step 3800/10000; acc: 37.1; ppl: 159.2; xent: 5.1; lr: 0.00013; sents:   14074; bsz:  947/ 911/35; 3377/3247 tok/s;   4552 sec;\n",
            "[2023-09-22 02:59:35,803 INFO] Step 3900/10000; acc: 37.3; ppl: 156.7; xent: 5.1; lr: 0.00014; sents:   14600; bsz:  946/ 911/36; 3313/3192 tok/s;   4666 sec;\n",
            "[2023-09-22 03:01:27,890 INFO] Step 4000/10000; acc: 39.3; ppl: 137.6; xent: 4.9; lr: 0.00014; sents:   15977; bsz:  943/ 908/40; 3366/3240 tok/s;   4779 sec;\n",
            "[2023-09-22 03:02:50,030 INFO] valid stats calculation\n",
            "                           took: 82.13594484329224 s.\n",
            "[2023-09-22 03:02:50,034 INFO] Train perplexity: 749.906\n",
            "[2023-09-22 03:02:50,034 INFO] Train accuracy: 22.8015\n",
            "[2023-09-22 03:02:50,034 INFO] Sentences processed: 599961\n",
            "[2023-09-22 03:02:50,034 INFO] Average bsz:  945/ 910/37\n",
            "[2023-09-22 03:02:50,034 INFO] Validation perplexity: 148.357\n",
            "[2023-09-22 03:02:50,034 INFO] Validation accuracy: 41.1511\n",
            "[2023-09-22 03:02:50,034 INFO] Model is improving ppl: inf --> 148.357.\n",
            "[2023-09-22 03:02:50,034 INFO] Model is improving acc: -inf --> 41.1511.\n",
            "[2023-09-22 03:02:50,053 INFO] Saving checkpoint models/model.engswa_step_4000.pt\n",
            "[2023-09-22 03:06:12,715 INFO] Step 4100/10000; acc: 39.2; ppl: 138.1; xent: 4.9; lr: 0.00014; sents:   15351; bsz:  944/ 908/38; 1326/1275 tok/s;   5063 sec;\n",
            "[2023-09-22 03:08:05,746 INFO] Step 4200/10000; acc: 40.1; ppl: 129.8; xent: 4.9; lr: 0.00015; sents:   14585; bsz:  945/ 913/36; 3346/3230 tok/s;   5176 sec;\n",
            "[2023-09-22 03:09:58,125 INFO] Step 4300/10000; acc: 40.4; ppl: 126.0; xent: 4.8; lr: 0.00015; sents:   15582; bsz:  943/ 910/39; 3357/3237 tok/s;   5289 sec;\n",
            "[2023-09-22 03:11:50,490 INFO] Step 4400/10000; acc: 41.3; ppl: 116.6; xent: 4.8; lr: 0.00016; sents:   15065; bsz:  944/ 909/38; 3361/3235 tok/s;   5401 sec;\n",
            "[2023-09-22 03:13:42,831 INFO] Step 4500/10000; acc: 42.7; ppl: 107.4; xent: 4.7; lr: 0.00016; sents:   15268; bsz:  946/ 908/38; 3368/3234 tok/s;   5514 sec;\n",
            "[2023-09-22 03:15:35,290 INFO] Step 4600/10000; acc: 43.1; ppl: 103.5; xent: 4.6; lr: 0.00016; sents:   15511; bsz:  943/ 909/39; 3354/3232 tok/s;   5626 sec;\n",
            "[2023-09-22 03:17:27,833 INFO] Step 4700/10000; acc: 43.4; ppl: 100.2; xent: 4.6; lr: 0.00017; sents:   14948; bsz:  944/ 911/37; 3355/3238 tok/s;   5739 sec;\n",
            "[2023-09-22 03:19:20,068 INFO] Step 4800/10000; acc: 44.7; ppl:  90.9; xent: 4.5; lr: 0.00017; sents:   14246; bsz:  948/ 912/36; 3379/3251 tok/s;   5851 sec;\n",
            "[2023-09-22 03:19:53,393 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=11027)\n",
            "\n",
            "[2023-09-22 03:19:53,393 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 10\n",
            "[2023-09-22 03:20:02,245 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 11\n",
            "[2023-09-22 03:21:47,787 INFO] Step 4900/10000; acc: 47.8; ppl:  69.8; xent: 4.2; lr: 0.00017; sents:   14631; bsz:  947/ 908/37; 2564/2460 tok/s;   5998 sec;\n",
            "[2023-09-22 03:23:40,049 INFO] Step 5000/10000; acc: 48.4; ppl:  66.4; xent: 4.2; lr: 0.00018; sents:   15292; bsz:  945/ 909/38; 3367/3239 tok/s;   6111 sec;\n",
            "[2023-09-22 03:23:40,081 INFO] Saving checkpoint models/model.engswa_step_5000.pt\n",
            "[2023-09-22 03:27:16,618 INFO] Step 5100/10000; acc: 48.7; ppl:  65.5; xent: 4.2; lr: 0.00018; sents:   14290; bsz:  948/ 911/36; 1751/1683 tok/s;   6327 sec;\n",
            "[2023-09-22 03:29:07,516 INFO] Step 5200/10000; acc: 48.8; ppl:  64.9; xent: 4.2; lr: 0.00017; sents:   15585; bsz:  943/ 907/39; 3401/3272 tok/s;   6438 sec;\n",
            "[2023-09-22 03:30:58,356 INFO] Step 5300/10000; acc: 48.9; ppl:  63.6; xent: 4.2; lr: 0.00017; sents:   14467; bsz:  943/ 910/36; 3404/3283 tok/s;   6549 sec;\n",
            "[2023-09-22 03:32:49,247 INFO] Step 5400/10000; acc: 51.0; ppl:  56.5; xent: 4.0; lr: 0.00017; sents:   15065; bsz:  946/ 910/38; 3411/3283 tok/s;   6660 sec;\n",
            "[2023-09-22 03:34:40,265 INFO] Step 5500/10000; acc: 50.8; ppl:  56.7; xent: 4.0; lr: 0.00017; sents:   15366; bsz:  944/ 912/38; 3400/3287 tok/s;   6771 sec;\n",
            "[2023-09-22 03:36:31,610 INFO] Step 5600/10000; acc: 52.5; ppl:  51.1; xent: 3.9; lr: 0.00017; sents:   15529; bsz:  944/ 911/39; 3393/3272 tok/s;   6882 sec;\n",
            "[2023-09-22 03:38:22,665 INFO] Step 5700/10000; acc: 53.1; ppl:  49.3; xent: 3.9; lr: 0.00017; sents:   15056; bsz:  945/ 908/38; 3404/3270 tok/s;   6993 sec;\n",
            "[2023-09-22 03:40:13,783 INFO] Step 5800/10000; acc: 53.9; ppl:  46.8; xent: 3.8; lr: 0.00016; sents:   15109; bsz:  945/ 911/38; 3401/3281 tok/s;   7104 sec;\n",
            "[2023-09-22 03:42:04,764 INFO] Step 5900/10000; acc: 53.5; ppl:  46.8; xent: 3.8; lr: 0.00016; sents:   14894; bsz:  945/ 909/37; 3406/3276 tok/s;   7215 sec;\n",
            "[2023-09-22 03:43:55,724 INFO] Step 6000/10000; acc: 55.8; ppl:  41.3; xent: 3.7; lr: 0.00016; sents:   14751; bsz:  945/ 910/37; 3407/3281 tok/s;   7326 sec;\n",
            "[2023-09-22 03:43:55,745 INFO] Saving checkpoint models/model.engswa_step_6000.pt\n",
            "[2023-09-22 03:46:27,461 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=11028)\n",
            "\n",
            "[2023-09-22 03:46:27,462 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 12\n",
            "[2023-09-22 03:46:36,891 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 13\n",
            "[2023-09-22 03:48:07,729 INFO] Step 6100/10000; acc: 58.7; ppl:  33.2; xent: 3.5; lr: 0.00016; sents:   14795; bsz:  945/ 912/37; 1500/1447 tok/s;   7578 sec;\n",
            "[2023-09-22 03:49:57,153 INFO] Step 6200/10000; acc: 60.4; ppl:  29.8; xent: 3.4; lr: 0.00016; sents:   14364; bsz:  946/ 910/36; 3459/3327 tok/s;   7688 sec;\n",
            "[2023-09-22 03:51:47,467 INFO] Step 6300/10000; acc: 61.3; ppl:  28.4; xent: 3.3; lr: 0.00016; sents:   14434; bsz:  946/ 912/36; 3431/3307 tok/s;   7798 sec;\n",
            "[2023-09-22 03:53:40,809 INFO] Step 6400/10000; acc: 61.0; ppl:  28.8; xent: 3.4; lr: 0.00016; sents:   15498; bsz:  945/ 908/39; 3334/3204 tok/s;   7911 sec;\n",
            "[2023-09-22 03:55:32,712 INFO] Step 6500/10000; acc: 61.4; ppl:  28.4; xent: 3.3; lr: 0.00016; sents:   15355; bsz:  946/ 907/38; 3381/3242 tok/s;   8023 sec;\n",
            "[2023-09-22 03:57:24,899 INFO] Step 6600/10000; acc: 62.0; ppl:  27.5; xent: 3.3; lr: 0.00015; sents:   15525; bsz:  943/ 910/39; 3363/3246 tok/s;   8136 sec;\n",
            "[2023-09-22 03:59:16,972 INFO] Step 6700/10000; acc: 62.9; ppl:  26.3; xent: 3.3; lr: 0.00015; sents:   15513; bsz:  947/ 908/39; 3380/3242 tok/s;   8248 sec;\n",
            "[2023-09-22 04:01:08,818 INFO] Step 6800/10000; acc: 62.9; ppl:  26.1; xent: 3.3; lr: 0.00015; sents:   15128; bsz:  946/ 911/38; 3383/3257 tok/s;   8360 sec;\n",
            "[2023-09-22 04:03:00,921 INFO] Step 6900/10000; acc: 62.6; ppl:  26.4; xent: 3.3; lr: 0.00015; sents:   15339; bsz:  941/ 912/38; 3359/3256 tok/s;   8472 sec;\n",
            "[2023-09-22 04:04:53,102 INFO] Step 7000/10000; acc: 63.5; ppl:  25.2; xent: 3.2; lr: 0.00015; sents:   15088; bsz:  944/ 908/38; 3368/3239 tok/s;   8584 sec;\n",
            "[2023-09-22 04:04:53,122 INFO] Saving checkpoint models/model.engswa_step_7000.pt\n",
            "[2023-09-22 04:08:30,095 INFO] Step 7100/10000; acc: 64.4; ppl:  23.9; xent: 3.2; lr: 0.00015; sents:   14283; bsz:  948/ 907/36; 1747/1672 tok/s;   8801 sec;\n",
            "[2023-09-22 04:10:22,202 INFO] Step 7200/10000; acc: 64.5; ppl:  23.6; xent: 3.2; lr: 0.00015; sents:   14810; bsz:  943/ 911/37; 3364/3252 tok/s;   8913 sec;\n",
            "[2023-09-22 04:11:08,458 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=11029)\n",
            "\n",
            "[2023-09-22 04:11:08,458 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 14\n",
            "[2023-09-22 04:11:17,901 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 15\n",
            "[2023-09-22 04:12:45,058 INFO] Step 7300/10000; acc: 68.4; ppl:  19.1; xent: 2.9; lr: 0.00015; sents:   14524; bsz:  946/ 910/36; 2650/2547 tok/s;   9056 sec;\n",
            "[2023-09-22 04:14:37,546 INFO] Step 7400/10000; acc: 71.2; ppl:  16.5; xent: 2.8; lr: 0.00015; sents:   15065; bsz:  943/ 909/38; 3353/3234 tok/s;   9168 sec;\n",
            "[2023-09-22 04:16:29,874 INFO] Step 7500/10000; acc: 71.8; ppl:  16.1; xent: 2.8; lr: 0.00014; sents:   15053; bsz:  945/ 910/38; 3366/3240 tok/s;   9281 sec;\n",
            "[2023-09-22 04:18:22,271 INFO] Step 7600/10000; acc: 72.3; ppl:  15.6; xent: 2.7; lr: 0.00014; sents:   15033; bsz:  949/ 910/38; 3376/3239 tok/s;   9393 sec;\n",
            "[2023-09-22 04:20:14,770 INFO] Step 7700/10000; acc: 72.3; ppl:  15.7; xent: 2.8; lr: 0.00014; sents:   15123; bsz:  943/ 910/38; 3353/3236 tok/s;   9505 sec;\n",
            "[2023-09-22 04:22:07,200 INFO] Step 7800/10000; acc: 72.4; ppl:  15.6; xent: 2.7; lr: 0.00014; sents:   14262; bsz:  946/ 910/36; 3367/3239 tok/s;   9618 sec;\n",
            "[2023-09-22 04:23:59,330 INFO] Step 7900/10000; acc: 72.8; ppl:  15.2; xent: 2.7; lr: 0.00014; sents:   15656; bsz:  945/ 909/39; 3373/3242 tok/s;   9730 sec;\n",
            "[2023-09-22 04:25:51,478 INFO] Step 8000/10000; acc: 73.8; ppl:  14.5; xent: 2.7; lr: 0.00014; sents:   15355; bsz:  942/ 912/38; 3359/3252 tok/s;   9842 sec;\n",
            "[2023-09-22 04:25:52,843 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=835)\n",
            "\n",
            "[2023-09-22 04:27:13,849 INFO] valid stats calculation\n",
            "                           took: 82.36451888084412 s.\n",
            "[2023-09-22 04:27:13,852 INFO] Train perplexity: 173.941\n",
            "[2023-09-22 04:27:13,852 INFO] Train accuracy: 39.761\n",
            "[2023-09-22 04:27:13,852 INFO] Sentences processed: 1.20076e+06\n",
            "[2023-09-22 04:27:13,852 INFO] Average bsz:  945/ 910/38\n",
            "[2023-09-22 04:27:13,853 INFO] Validation perplexity: 65.511\n",
            "[2023-09-22 04:27:13,853 INFO] Validation accuracy: 59.0969\n",
            "[2023-09-22 04:27:13,853 INFO] Model is improving ppl: 148.357 --> 65.511.\n",
            "[2023-09-22 04:27:13,853 INFO] Model is improving acc: 41.1511 --> 59.0969.\n",
            "[2023-09-22 04:27:13,871 INFO] Saving checkpoint models/model.engswa_step_8000.pt\n",
            "[2023-09-22 04:30:22,225 INFO] Step 8100/10000; acc: 74.1; ppl:  14.4; xent: 2.7; lr: 0.00014; sents:   14716; bsz:  944/ 911/37; 1395/1346 tok/s;  10113 sec;\n",
            "[2023-09-22 04:32:14,202 INFO] Step 8200/10000; acc: 73.2; ppl:  14.9; xent: 2.7; lr: 0.00014; sents:   14930; bsz:  945/ 910/37; 3375/3252 tok/s;  10225 sec;\n",
            "[2023-09-22 04:34:06,538 INFO] Step 8300/10000; acc: 74.5; ppl:  14.1; xent: 2.6; lr: 0.00014; sents:   15551; bsz:  945/ 908/39; 3364/3232 tok/s;  10337 sec;\n",
            "[2023-09-22 04:35:58,403 INFO] Step 8400/10000; acc: 75.3; ppl:  13.6; xent: 2.6; lr: 0.00014; sents:   14824; bsz:  944/ 909/37; 3377/3252 tok/s;  10449 sec;\n",
            "[2023-09-22 04:36:54,866 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=11027)\n",
            "\n",
            "[2023-09-22 04:36:54,866 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 16\n",
            "[2023-09-22 04:37:03,556 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 17\n",
            "[2023-09-22 04:38:24,812 INFO] Step 8500/10000; acc: 78.8; ppl:  11.6; xent: 2.4; lr: 0.00014; sents:   14461; bsz:  948/ 907/36; 2589/2477 tok/s;  10595 sec;\n",
            "[2023-09-22 04:40:16,852 INFO] Step 8600/10000; acc: 82.2; ppl:  10.1; xent: 2.3; lr: 0.00013; sents:   14649; bsz:  948/ 911/37; 3384/3253 tok/s;  10708 sec;\n",
            "[2023-09-22 04:42:08,960 INFO] Step 8700/10000; acc: 82.3; ppl:  10.0; xent: 2.3; lr: 0.00013; sents:   14555; bsz:  948/ 912/36; 3381/3253 tok/s;  10820 sec;\n",
            "[2023-09-22 04:44:00,884 INFO] Step 8800/10000; acc: 81.7; ppl:  10.2; xent: 2.3; lr: 0.00013; sents:   15075; bsz:  945/ 908/38; 3377/3245 tok/s;  10932 sec;\n",
            "[2023-09-22 04:45:52,931 INFO] Step 8900/10000; acc: 82.0; ppl:  10.1; xent: 2.3; lr: 0.00013; sents:   15993; bsz:  944/ 909/40; 3370/3243 tok/s;  11044 sec;\n",
            "[2023-09-22 04:47:45,217 INFO] Step 9000/10000; acc: 82.5; ppl:   9.8; xent: 2.3; lr: 0.00013; sents:   14743; bsz:  946/ 910/37; 3370/3241 tok/s;  11156 sec;\n",
            "[2023-09-22 04:47:45,249 INFO] Saving checkpoint models/model.engswa_step_9000.pt\n",
            "[2023-09-22 04:51:27,653 INFO] Step 9100/10000; acc: 81.5; ppl:  10.2; xent: 2.3; lr: 0.00013; sents:   15412; bsz:  944/ 910/39; 1698/1637 tok/s;  11378 sec;\n",
            "[2023-09-22 04:53:20,020 INFO] Step 9200/10000; acc: 83.6; ppl:   9.4; xent: 2.2; lr: 0.00013; sents:   14835; bsz:  947/ 912/37; 3371/3248 tok/s;  11491 sec;\n",
            "[2023-09-22 04:55:12,142 INFO] Step 9300/10000; acc: 82.3; ppl:   9.9; xent: 2.3; lr: 0.00013; sents:   15498; bsz:  944/ 908/39; 3366/3241 tok/s;  11603 sec;\n",
            "[2023-09-22 04:57:04,121 INFO] Step 9400/10000; acc: 82.7; ppl:   9.7; xent: 2.3; lr: 0.00013; sents:   14775; bsz:  943/ 911/37; 3367/3254 tok/s;  11715 sec;\n",
            "[2023-09-22 04:58:56,634 INFO] Step 9500/10000; acc: 82.8; ppl:   9.6; xent: 2.3; lr: 0.00013; sents:   14566; bsz:  944/ 907/36; 3357/3226 tok/s;  11827 sec;\n",
            "[2023-09-22 05:00:48,748 INFO] Step 9600/10000; acc: 83.8; ppl:   9.3; xent: 2.2; lr: 0.00013; sents:   15596; bsz:  942/ 911/39; 3360/3252 tok/s;  11939 sec;\n",
            "[2023-09-22 05:01:48,469 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=11028)\n",
            "\n",
            "[2023-09-22 05:01:48,470 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 18\n",
            "[2023-09-22 05:01:56,816 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 19\n",
            "[2023-09-22 05:03:11,232 INFO] Step 9700/10000; acc: 86.6; ppl:   8.3; xent: 2.1; lr: 0.00013; sents:   14959; bsz:  945/ 909/37; 2654/2552 tok/s;  12082 sec;\n",
            "[2023-09-22 05:05:03,329 INFO] Step 9800/10000; acc: 88.9; ppl:   7.6; xent: 2.0; lr: 0.00013; sents:   14807; bsz:  945/ 908/37; 3373/3242 tok/s;  12194 sec;\n",
            "[2023-09-22 05:06:55,427 INFO] Step 9900/10000; acc: 89.3; ppl:   7.5; xent: 2.0; lr: 0.00013; sents:   15740; bsz:  942/ 911/39; 3363/3252 tok/s;  12306 sec;\n",
            "[2023-09-22 05:08:46,997 INFO] Step 10000/10000; acc: 89.1; ppl:   7.5; xent: 2.0; lr: 0.00012; sents:   14867; bsz:  946/ 910/37; 3392/3263 tok/s;  12418 sec;\n",
            "[2023-09-22 05:08:47,018 INFO] Saving checkpoint models/model.engswa_step_10000.pt\n"
          ]
        }
      ],
      "source": [
        " # Train the NMT model\n",
        "!onmt_train -config /content/config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx9_vrq0gC6a",
        "outputId": "81b1975a-2444-400b-8f97-1d689f3d5985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-09-22 05:10:49.042187: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-22 05:10:51.058130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-09-22 05:10:54.019271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-22 05:10:54.019793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-22 05:10:54.020012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2023-09-22 05:10:54,899 INFO] Loading checkpoint from /content/models/model.engswa_step_10000.pt\n",
            "[2023-09-22 05:11:24,042 INFO] Loading data into the model\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_translate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/translate.py\", line 56, in main\n",
            "    translate(opt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/translate.py\", line 37, in translate\n",
            "    _, _ = translator._translate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/translate/translator.py\", line 442, in _translate\n",
            "    self.out_file.flush()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        }
      ],
      "source": [
        "!onmt_translate -model /content/models/model.engswa_step_15000.pt -src /content/drive/MyDrive/EngSwaBBC/cleanGoURMETEng.txt.subword.test -output /content/drive/MyDrive/EngSwaBBC/GoURMETSwa_Translated_V4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9T70zQfDhT0w",
        "outputId": "6d9df2f1-507b-4bc4-ea23-14c3768335dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/nmt\n",
            "Cloning into 'MT-Preparation'...\n",
            "remote: Enumerating objects: 248, done.\u001b[K\n",
            "remote: Counting objects: 100% (248/248), done.\u001b[K\n",
            "remote: Compressing objects: 100% (139/139), done.\u001b[K\n",
            "remote: Total 248 (delta 123), reused 193 (delta 97), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (248/248), 64.32 KiB | 4.02 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p nmt\n",
        "%cd nmt\n",
        "!git clone https://github.com/ymoslem/MT-Preparation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8g6so3PXh8Kc"
      },
      "outputs": [],
      "source": [
        "# Install the requirements\n",
        "!pip3 install -r MT-Preparation/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "US7HA0aciCeP"
      },
      "outputs": [],
      "source": [
        "%cd  ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_mdE7WYWjN1_"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j5UXPziljgVp"
      },
      "outputs": [],
      "source": [
        "!python3 nmt/MT-Preparation/subwording/3-desubword.py /content/drive/MyDrive/EngSwaBBC/target.model /content/drive/MyDrive/EngSwaBBC/GoURMETSwa_Translated_V4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zHTXW0P3kw0K"
      },
      "outputs": [],
      "source": [
        "!python3 nmt/MT-Preparation/subwording/3-desubword.py /content/drive/MyDrive/EngSwaBBC/target.model /content/drive/MyDrive/EngSwaBBC/cleanGoURMETSwa.txt.subword.test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zcDtaHujn3P",
        "outputId": "8d052f74-3ec5-4350-b693-c8fd2e1df6ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-22 07:00:09--  https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 957 [text/plain]\n",
            "Saving to: ‘compute-bleu.py.1’\n",
            "\n",
            "compute-bleu.py.1   100%[===================>]     957  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-22 07:00:09 (45.6 MB/s) - ‘compute-bleu.py.1’ saved [957/957]\n",
            "\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Download the BLEU script\n",
        "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
        "!pip3 install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cys-wvvakOmQ",
        "outputId": "dca7b644-00c0-4997-f931-50737eb1ea71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1st sentence: Wale walioingia kwenye safina waliokolewa, wakati wale waliokataa kufuata maelekezo ya Mungu waliachwa nje.\n",
            "MTed 1st sentence: Wale walioingia kwenye safina waliokolewa, wakati wale waliokataa kufuata maelekezo ya Mungu waliachwa nje.\n",
            "BLEU:  37.08511655662515\n"
          ]
        }
      ],
      "source": [
        "!python3 compute-bleu.py /content/drive/MyDrive/EngSwaBBC/cleanGoURMETSwa.txt.subword.test.desubword /content/drive/MyDrive/EngSwaBBC/GoURMETSwa_Translated_V4.desubword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0C1zRojARqc3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2p9HsIvpmIve"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}