{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKKD2bzWRf31",
        "outputId": "778717a2-c93f-4042-9064-fb2451b558fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUiZSmd-nw8S",
        "outputId": "51bd253f-d5e6-46f0-bc0b-9e3019840b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package python-pil\n",
            "E: Unable to locate package python-lxml\n"
          ]
        }
      ],
      "source": [
        "!apt-get -y install protobuf-compiler python-pil python-lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLEFcu7tn5du",
        "outputId": "c57e9522-ec4a-4e9e-a1fd-420021d86ead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openNMT-py\n",
            "  Downloading OpenNMT_py-3.3-py3-none-any.whl (242 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/242.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/242.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.9/242.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<2.1,>=1.13 in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (2.0.1+cu118)\n",
            "Collecting configargparse (from openNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting ctranslate2<4,>=3.2 (from openNMT-py)\n",
            "  Downloading ctranslate2-3.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (2.12.3)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (2.2.5)\n",
            "Collecting waitress (from openNMT-py)\n",
            "  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.35 (from openNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from openNMT-py) (6.0.1)\n",
            "Collecting sacrebleu (from openNMT-py)\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from openNMT-py)\n",
            "  Downloading rapidfuzz-3.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from openNMT-py)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel (from openNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.2->openNMT-py) (1.22.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (1.56.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (3.4.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->openNMT-py) (0.41.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->openNMT-py) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->openNMT-py) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->openNMT-py) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->openNMT-py) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->openNMT-py) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->openNMT-py) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=1.13->openNMT-py) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=1.13->openNMT-py) (16.0.6)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel->openNMT-py)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->openNMT-py) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->openNMT-py) (8.1.6)\n",
            "Collecting portalocker (from sacrebleu->openNMT-py)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->openNMT-py) (2022.10.31)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->openNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->openNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->openNMT-py) (4.9.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->openNMT-py) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->openNMT-py) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->openNMT-py) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->openNMT-py) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->openNMT-py) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.1,>=1.13->openNMT-py) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->openNMT-py) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->openNMT-py) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->openNMT-py) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->openNMT-py) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.1,>=1.13->openNMT-py) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->openNMT-py) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->openNMT-py) (3.2.2)\n",
            "Installing collected packages: waitress, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, ctranslate2, configargparse, colorama, sacrebleu, fasttext-wheel, openNMT-py\n",
            "Successfully installed colorama-0.4.6 configargparse-1.7 ctranslate2-3.17.1 fasttext-wheel-0.9.2 openNMT-py-3.3 portalocker-2.7.0 pyahocorasick-2.0.0 pybind11-2.11.1 pyonmttok-1.37.1 rapidfuzz-3.1.2 sacrebleu-2.3.1 waitress-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openNMT-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3BPgqwFl9PH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-sDfaoRoKC8"
      },
      "outputs": [],
      "source": [
        "config = '''# config.yaml\n",
        "\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# Training files\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/English_Tsonga_English_Parralel_train.txt.subword\n",
        "        path_tgt: /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/English_Tsonga_Tsonga_Parralel_train.txt.subword\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/English_Tsonga_English_dev.txt.subword\n",
        "        path_tgt: /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/English_Tsonga_Tsonga_dev.txt.subword\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabulary files, generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# Vocabulary size - should be the same as in sentence piece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 64\n",
        "tgt_seq_length: 64\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/source.model\n",
        "tgt_subword_model: /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/target.model\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model.engtsonga\n",
        "\n",
        "# Stop training if it does not imporve after n validations\n",
        "early_stopping: 10\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 2023\n",
        "\n",
        "# Default: 100000 - Train the model to max n steps\n",
        "# Increase to 200000 or more for large datasets\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 6000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 4000\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 5000\n",
        "report_every: 100\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 1024  # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 1024\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 0.4\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 16\n",
        "dec_layers: 16\n",
        "heads: 16\n",
        "hidden_size: 1024\n",
        "word_vec_size: 1024\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lNRjFUyFMl9",
        "outputId": "0d816ab3-f874-4c1e-a203-45ada3bc16f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2023-07-30 19:14:54,245 INFO] Counter vocab from -1 samples.\n",
            "[2023-07-30 19:14:54,246 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2023-07-30 19:15:07,949 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=763)\n",
            "\n",
            "[2023-07-30 19:15:08,024 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=727)\n",
            "\n",
            "[2023-07-30 19:15:08,087 INFO] Counters src: 31651\n",
            "[2023-07-30 19:15:08,087 INFO] Counters tgt: 37469\n"
          ]
        }
      ],
      "source": [
        "!onmt_build_vocab -config /content/config.yaml -n_sample -1 -num_threads 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFY_xrILFTPL",
        "outputId": "656bb3d2-b31a-4dbb-b208-9380f257fcb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-30 19:15:11,461 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2023-07-30 19:15:11,461 INFO] Parsed 2 corpora from -data.\n",
            "[2023-07-30 19:15:11,462 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2023-07-30 19:15:11,696 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁.', '▁the', '▁of', '▁to', '▁a', '▁and']\n",
            "[2023-07-30 19:15:11,696 INFO] The decoder start token is: <s>\n",
            "[2023-07-30 19:15:11,697 INFO] Building model...\n",
            "[2023-07-30 19:15:19,165 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2023-07-30 19:15:19,165 INFO] Non quantized layer compute is fp16\n",
            "[2023-07-30 19:15:24,681 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(31656, 1024, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-15): 16 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(37480, 1024, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-15): 16 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=1024, out_features=37480, bias=True)\n",
            ")\n",
            "[2023-07-30 19:15:24,687 INFO] encoder: 166701056\n",
            "[2023-07-30 19:15:24,688 INFO] decoder: 278223464\n",
            "[2023-07-30 19:15:24,688 INFO] * number of parameters: 444924520\n",
            "[2023-07-30 19:15:24,689 INFO] Trainable parameters = {'torch.float32': 444924520, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2023-07-30 19:15:24,689 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2023-07-30 19:15:24,690 INFO]  * src vocab size = 31656\n",
            "[2023-07-30 19:15:24,690 INFO]  * tgt vocab size = 37480\n",
            "[2023-07-30 19:15:24,694 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2023-07-30 19:15:24,694 INFO] Starting training on GPU: [0]\n",
            "[2023-07-30 19:15:24,694 INFO] Start training loop and validate every 4000 steps...\n",
            "[2023-07-30 19:15:24,694 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=64, tgt_seq_length=64))\n",
            "[2023-07-30 19:17:27,675 INFO] Step 100/ 6000; acc: 6.0; ppl: 23757.4; xent: 10.1; lr: 0.00000; sents:   28569; bsz:  721/ 937/71; 2345/3048 tok/s;    123 sec;\n",
            "[2023-07-30 19:19:17,825 INFO] Step 200/ 6000; acc: 7.4; ppl: 11065.5; xent: 9.3; lr: 0.00001; sents:   27647; bsz:  721/ 940/69; 2617/3412 tok/s;    233 sec;\n",
            "[2023-07-30 19:21:07,844 INFO] Step 300/ 6000; acc: 7.8; ppl: 4711.6; xent: 8.5; lr: 0.00001; sents:   28574; bsz:  715/ 937/71; 2599/3405 tok/s;    343 sec;\n",
            "[2023-07-30 19:22:57,725 INFO] Step 400/ 6000; acc: 9.9; ppl: 1787.7; xent: 7.5; lr: 0.00001; sents:   27652; bsz:  721/ 940/69; 2625/3421 tok/s;    453 sec;\n",
            "[2023-07-30 19:24:47,445 INFO] Step 500/ 6000; acc: 13.7; ppl: 948.0; xent: 6.9; lr: 0.00002; sents:   29491; bsz:  712/ 935/74; 2596/3409 tok/s;    563 sec;\n",
            "[2023-07-30 19:26:37,151 INFO] Step 600/ 6000; acc: 14.7; ppl: 753.5; xent: 6.6; lr: 0.00002; sents:   29740; bsz:  715/ 934/74; 2605/3406 tok/s;    672 sec;\n",
            "[2023-07-30 19:27:23,909 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1261)\n",
            "\n",
            "[2023-07-30 19:27:23,910 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2023-07-30 19:28:46,746 INFO] Step 700/ 6000; acc: 15.6; ppl: 648.6; xent: 6.5; lr: 0.00002; sents:   29381; bsz:  714/ 934/73; 2205/2884 tok/s;    802 sec;\n",
            "[2023-07-30 19:30:36,742 INFO] Step 800/ 6000; acc: 17.4; ppl: 522.9; xent: 6.3; lr: 0.00003; sents:   28107; bsz:  720/ 938/70; 2620/3412 tok/s;    912 sec;\n",
            "[2023-07-30 19:32:26,699 INFO] Step 900/ 6000; acc: 21.3; ppl: 405.9; xent: 6.0; lr: 0.00003; sents:   29829; bsz:  715/ 935/75; 2600/3400 tok/s;   1022 sec;\n",
            "[2023-07-30 19:34:17,092 INFO] Step 1000/ 6000; acc: 24.5; ppl: 307.4; xent: 5.7; lr: 0.00004; sents:   26645; bsz:  724/ 941/67; 2623/3409 tok/s;   1132 sec;\n",
            "[2023-07-30 19:34:17,117 INFO] Saving checkpoint models/model.engtsonga_step_1000.pt\n",
            "[2023-07-30 19:36:49,747 INFO] Step 1100/ 6000; acc: 28.2; ppl: 233.3; xent: 5.5; lr: 0.00004; sents:   28515; bsz:  716/ 937/71; 1876/2455 tok/s;   1285 sec;\n",
            "[2023-07-30 19:38:40,113 INFO] Step 1200/ 6000; acc: 30.3; ppl: 191.8; xent: 5.3; lr: 0.00004; sents:   27129; bsz:  722/ 941/68; 2616/3409 tok/s;   1395 sec;\n",
            "[2023-07-30 19:40:12,629 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1239)\n",
            "\n",
            "[2023-07-30 19:40:12,629 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2023-07-30 19:40:51,042 INFO] Step 1300/ 6000; acc: 33.0; ppl: 160.5; xent: 5.1; lr: 0.00005; sents:   29560; bsz:  713/ 935/74; 2177/2855 tok/s;   1526 sec;\n",
            "[2023-07-30 19:42:41,405 INFO] Step 1400/ 6000; acc: 35.4; ppl: 133.6; xent: 4.9; lr: 0.00005; sents:   31480; bsz:  709/ 930/79; 2568/3371 tok/s;   1637 sec;\n",
            "[2023-07-30 19:44:31,901 INFO] Step 1500/ 6000; acc: 37.1; ppl: 114.6; xent: 4.7; lr: 0.00005; sents:   26427; bsz:  724/ 941/66; 2620/3408 tok/s;   1747 sec;\n",
            "[2023-07-30 19:46:22,245 INFO] Step 1600/ 6000; acc: 38.9; ppl: 100.9; xent: 4.6; lr: 0.00006; sents:   27725; bsz:  721/ 938/69; 2613/3401 tok/s;   1858 sec;\n",
            "[2023-07-30 19:48:12,828 INFO] Step 1700/ 6000; acc: 40.4; ppl:  90.3; xent: 4.5; lr: 0.00006; sents:   28533; bsz:  721/ 938/71; 2609/3393 tok/s;   1968 sec;\n",
            "[2023-07-30 19:50:03,117 INFO] Step 1800/ 6000; acc: 43.3; ppl:  76.0; xent: 4.3; lr: 0.00006; sents:   27468; bsz:  721/ 939/69; 2614/3407 tok/s;   2078 sec;\n",
            "[2023-07-30 19:51:53,407 INFO] Step 1900/ 6000; acc: 45.3; ppl:  69.1; xent: 4.2; lr: 0.00007; sents:   30043; bsz:  708/ 934/75; 2569/3387 tok/s;   2189 sec;\n",
            "[2023-07-30 19:52:21,214 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1222)\n",
            "\n",
            "[2023-07-30 19:52:21,214 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2023-07-30 19:54:05,542 INFO] Step 2000/ 6000; acc: 48.3; ppl:  56.2; xent: 4.0; lr: 0.00007; sents:   26981; bsz:  723/ 941/67; 2188/2847 tok/s;   2321 sec;\n",
            "[2023-07-30 19:54:05,559 INFO] Saving checkpoint models/model.engtsonga_step_2000.pt\n",
            "[2023-07-30 19:56:33,250 INFO] Step 2100/ 6000; acc: 50.3; ppl:  51.2; xent: 3.9; lr: 0.00007; sents:   28771; bsz:  715/ 936/72; 1936/2535 tok/s;   2469 sec;\n",
            "[2023-07-30 19:58:23,583 INFO] Step 2200/ 6000; acc: 52.4; ppl:  45.7; xent: 3.8; lr: 0.00008; sents:   30700; bsz:  713/ 932/77; 2586/3380 tok/s;   2579 sec;\n",
            "[2023-07-30 20:00:13,923 INFO] Step 2300/ 6000; acc: 54.3; ppl:  41.2; xent: 3.7; lr: 0.00008; sents:   27896; bsz:  717/ 939/70; 2599/3402 tok/s;   2689 sec;\n",
            "[2023-07-30 20:02:04,371 INFO] Step 2400/ 6000; acc: 55.7; ppl:  37.8; xent: 3.6; lr: 0.00008; sents:   28634; bsz:  719/ 937/72; 2604/3395 tok/s;   2800 sec;\n",
            "[2023-07-30 20:03:54,674 INFO] Step 2500/ 6000; acc: 58.4; ppl:  32.8; xent: 3.5; lr: 0.00009; sents:   29114; bsz:  718/ 937/73; 2603/3398 tok/s;   2910 sec;\n",
            "[2023-07-30 20:05:09,245 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1236)\n",
            "\n",
            "[2023-07-30 20:05:09,245 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2023-07-30 20:06:04,185 INFO] Step 2600/ 6000; acc: 59.5; ppl:  30.4; xent: 3.4; lr: 0.00009; sents:   28470; bsz:  717/ 936/71; 2213/2892 tok/s;   3039 sec;\n",
            "[2023-07-30 20:07:54,338 INFO] Step 2700/ 6000; acc: 61.2; ppl:  27.5; xent: 3.3; lr: 0.00010; sents:   29455; bsz:  716/ 935/74; 2599/3396 tok/s;   3150 sec;\n",
            "[2023-07-30 20:09:44,791 INFO] Step 2800/ 6000; acc: 62.7; ppl:  25.1; xent: 3.2; lr: 0.00010; sents:   29430; bsz:  716/ 935/74; 2592/3387 tok/s;   3260 sec;\n",
            "[2023-07-30 20:11:34,988 INFO] Step 2900/ 6000; acc: 63.4; ppl:  24.1; xent: 3.2; lr: 0.00010; sents:   29436; bsz:  714/ 935/74; 2590/3394 tok/s;   3370 sec;\n",
            "[2023-07-30 20:13:25,481 INFO] Step 3000/ 6000; acc: 64.4; ppl:  22.9; xent: 3.1; lr: 0.00011; sents:   27574; bsz:  719/ 938/69; 2605/3397 tok/s;   3481 sec;\n",
            "[2023-07-30 20:13:25,498 INFO] Saving checkpoint models/model.engtsonga_step_3000.pt\n",
            "[2023-07-30 20:15:54,891 INFO] Step 3100/ 6000; acc: 65.7; ppl:  21.1; xent: 3.1; lr: 0.00011; sents:   27346; bsz:  722/ 940/68; 1932/2517 tok/s;   3630 sec;\n",
            "[2023-07-30 20:17:45,451 INFO] Step 3200/ 6000; acc: 66.9; ppl:  19.9; xent: 3.0; lr: 0.00011; sents:   26899; bsz:  722/ 940/67; 2613/3400 tok/s;   3741 sec;\n",
            "[2023-07-30 20:19:57,940 INFO] Step 3300/ 6000; acc: 68.8; ppl:  17.5; xent: 2.9; lr: 0.00012; sents:   27517; bsz:  722/ 940/69; 2179/2837 tok/s;   3873 sec;\n",
            "[2023-07-30 20:21:48,399 INFO] Step 3400/ 6000; acc: 69.1; ppl:  17.1; xent: 2.8; lr: 0.00012; sents:   27398; bsz:  720/ 940/68; 2607/3405 tok/s;   3984 sec;\n",
            "[2023-07-30 20:23:38,890 INFO] Step 3500/ 6000; acc: 69.7; ppl:  16.7; xent: 2.8; lr: 0.00012; sents:   27962; bsz:  719/ 938/70; 2601/3395 tok/s;   4094 sec;\n",
            "[2023-07-30 20:25:29,370 INFO] Step 3600/ 6000; acc: 69.7; ppl:  16.7; xent: 2.8; lr: 0.00013; sents:   28057; bsz:  719/ 937/70; 2602/3394 tok/s;   4205 sec;\n",
            "[2023-07-30 20:27:19,853 INFO] Step 3700/ 6000; acc: 70.5; ppl:  15.9; xent: 2.8; lr: 0.00013; sents:   28113; bsz:  721/ 939/70; 2611/3398 tok/s;   4315 sec;\n",
            "[2023-07-30 20:29:10,000 INFO] Step 3800/ 6000; acc: 70.7; ppl:  15.9; xent: 2.8; lr: 0.00013; sents:   30175; bsz:  711/ 934/75; 2580/3390 tok/s;   4425 sec;\n",
            "[2023-07-30 20:29:57,657 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2486)\n",
            "\n",
            "[2023-07-30 20:29:57,657 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2023-07-30 20:31:22,346 INFO] Step 3900/ 6000; acc: 72.4; ppl:  14.3; xent: 2.7; lr: 0.00014; sents:   30050; bsz:  713/ 934/75; 2156/2822 tok/s;   4558 sec;\n",
            "[2023-07-30 20:33:12,531 INFO] Step 4000/ 6000; acc: 73.1; ppl:  13.6; xent: 2.6; lr: 0.00014; sents:   29179; bsz:  714/ 934/73; 2591/3389 tok/s;   4668 sec;\n",
            "[2023-07-30 20:39:53,170 INFO] valid stats calculation and sentences rebuilding\n",
            "                           took: 400.6358950138092 s.\n",
            "[2023-07-30 20:39:53,173 INFO] Train perplexity: 98.1572\n",
            "[2023-07-30 20:39:53,173 INFO] Train accuracy: 44.9353\n",
            "[2023-07-30 20:39:53,174 INFO] Sentences processed: 1.14167e+06\n",
            "[2023-07-30 20:39:53,174 INFO] Average bsz:  717/ 937/71\n",
            "[2023-07-30 20:39:53,174 INFO] Validation perplexity: 11.6337\n",
            "[2023-07-30 20:39:53,174 INFO] Validation accuracy: 78.1841\n",
            "[2023-07-30 20:39:53,174 INFO] Model is improving ppl: inf --> 11.6337.\n",
            "[2023-07-30 20:39:53,174 INFO] Model is improving acc: -inf --> 78.1841.\n",
            "[2023-07-30 20:39:53,190 INFO] Saving checkpoint models/model.engtsonga_step_4000.pt\n",
            "[2023-07-30 20:42:22,771 INFO] Step 4100/ 6000; acc: 73.8; ppl:  13.0; xent: 2.6; lr: 0.00014; sents:   27021; bsz:  725/ 940/68; 527/683 tok/s;   5218 sec;\n",
            "[2023-07-30 20:44:13,316 INFO] Step 4200/ 6000; acc: 73.5; ppl:  13.3; xent: 2.6; lr: 0.00015; sents:   28153; bsz:  717/ 937/70; 2594/3392 tok/s;   5329 sec;\n",
            "[2023-07-30 20:46:03,521 INFO] Step 4300/ 6000; acc: 73.8; ppl:  13.1; xent: 2.6; lr: 0.00015; sents:   29540; bsz:  714/ 936/74; 2592/3396 tok/s;   5439 sec;\n",
            "[2023-07-30 20:47:53,866 INFO] Step 4400/ 6000; acc: 73.8; ppl:  13.1; xent: 2.6; lr: 0.00016; sents:   29493; bsz:  717/ 936/74; 2599/3391 tok/s;   5549 sec;\n",
            "[2023-07-30 20:49:29,558 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1263)\n",
            "\n",
            "[2023-07-30 20:49:29,558 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2023-07-30 20:50:04,762 INFO] Step 4500/ 6000; acc: 74.8; ppl:  12.4; xent: 2.5; lr: 0.00016; sents:   28300; bsz:  716/ 937/71; 2188/2864 tok/s;   5680 sec;\n",
            "[2023-07-30 20:51:55,089 INFO] Step 4600/ 6000; acc: 76.8; ppl:  11.0; xent: 2.4; lr: 0.00016; sents:   28891; bsz:  718/ 937/72; 2604/3399 tok/s;   5790 sec;\n",
            "[2023-07-30 20:53:45,375 INFO] Step 4700/ 6000; acc: 76.6; ppl:  11.1; xent: 2.4; lr: 0.00017; sents:   28768; bsz:  715/ 935/72; 2592/3392 tok/s;   5901 sec;\n",
            "[2023-07-30 20:55:35,694 INFO] Step 4800/ 6000; acc: 76.3; ppl:  11.3; xent: 2.4; lr: 0.00017; sents:   28439; bsz:  720/ 937/71; 2610/3396 tok/s;   6011 sec;\n",
            "[2023-07-30 20:57:26,011 INFO] Step 4900/ 6000; acc: 76.2; ppl:  11.4; xent: 2.4; lr: 0.00017; sents:   28213; bsz:  718/ 938/71; 2604/3402 tok/s;   6121 sec;\n",
            "[2023-07-30 20:59:16,426 INFO] Step 5000/ 6000; acc: 76.2; ppl:  11.3; xent: 2.4; lr: 0.00018; sents:   28088; bsz:  718/ 939/70; 2603/3400 tok/s;   6232 sec;\n",
            "[2023-07-30 20:59:16,443 INFO] Saving checkpoint models/model.engtsonga_step_5000.pt\n",
            "[2023-07-30 21:01:38,853 INFO] Step 5100/ 6000; acc: 76.7; ppl:  11.1; xent: 2.4; lr: 0.00018; sents:   28948; bsz:  717/ 937/72; 2015/2631 tok/s;   6374 sec;\n",
            "[2023-07-30 21:02:11,778 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1239)\n",
            "\n",
            "[2023-07-30 21:02:11,778 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2023-07-30 21:03:51,354 INFO] Step 5200/ 6000; acc: 79.2; ppl:   9.8; xent: 2.3; lr: 0.00017; sents:   28902; bsz:  717/ 936/72; 2163/2825 tok/s;   6507 sec;\n",
            "[2023-07-30 21:05:41,744 INFO] Step 5300/ 6000; acc: 79.4; ppl:   9.6; xent: 2.3; lr: 0.00017; sents:   26874; bsz:  723/ 941/67; 2618/3410 tok/s;   6617 sec;\n",
            "[2023-07-30 21:07:32,422 INFO] Step 5400/ 6000; acc: 79.3; ppl:   9.6; xent: 2.3; lr: 0.00017; sents:   27519; bsz:  721/ 939/69; 2607/3394 tok/s;   6728 sec;\n",
            "[2023-07-30 21:09:22,858 INFO] Step 5500/ 6000; acc: 79.1; ppl:   9.8; xent: 2.3; lr: 0.00017; sents:   31209; bsz:  707/ 932/78; 2562/3374 tok/s;   6838 sec;\n",
            "[2023-07-30 21:11:13,346 INFO] Step 5600/ 6000; acc: 79.1; ppl:   9.8; xent: 2.3; lr: 0.00017; sents:   28874; bsz:  717/ 935/72; 2597/3384 tok/s;   6949 sec;\n",
            "[2023-07-30 21:13:03,925 INFO] Step 5700/ 6000; acc: 78.8; ppl:   9.9; xent: 2.3; lr: 0.00017; sents:   27559; bsz:  717/ 939/69; 2594/3396 tok/s;   7059 sec;\n",
            "[2023-07-30 21:14:21,882 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1218)\n",
            "\n",
            "[2023-07-30 21:14:21,883 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2023-07-30 21:15:18,808 INFO] Step 5800/ 6000; acc: 80.2; ppl:   9.2; xent: 2.2; lr: 0.00016; sents:   29993; bsz:  712/ 936/75; 2112/2774 tok/s;   7194 sec;\n",
            "[2023-07-30 21:17:09,202 INFO] Step 5900/ 6000; acc: 82.1; ppl:   8.4; xent: 2.1; lr: 0.00016; sents:   27472; bsz:  720/ 939/69; 2608/3403 tok/s;   7305 sec;\n",
            "[2023-07-30 21:18:59,510 INFO] Step 6000/ 6000; acc: 81.8; ppl:   8.5; xent: 2.1; lr: 0.00016; sents:   27648; bsz:  721/ 939/69; 2616/3405 tok/s;   7415 sec;\n",
            "[2023-07-30 21:18:59,528 INFO] Saving checkpoint models/model.engtsonga_step_6000.pt\n"
          ]
        }
      ],
      "source": [
        " # Train the NMT model\n",
        "!onmt_train -config /content/config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Fx9_vrq0gC6a",
        "outputId": "dfe1644f-9e23-48ae-9fe6-f0fb6699c4d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-30 21:19:30,629 INFO] Loading checkpoint from /content/models/model.engtsonga_step_6000.pt\n",
            "[2023-07-30 21:19:43,795 INFO] Loading data into the model\n",
            "[2023-07-30 22:15:48,181 INFO] PRED SCORE: -0.2806, PRED PPL: 1.32 NB SENTENCES: 3000\n"
          ]
        }
      ],
      "source": [
        "!onmt_translate -model /content/models/model.engtsonga_step_6000.pt -src /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/English_Tsonga_English_test.txt.subword -output /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/Tsonga_Ukuxhumana.Translated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9T70zQfDhT0w",
        "outputId": "b29c1d54-6e81-42e2-f815-d1b3e031704d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/nmt\n",
            "Cloning into 'MT-Preparation'...\n",
            "remote: Enumerating objects: 239, done.\u001b[K\n",
            "remote: Counting objects: 100% (239/239), done.\u001b[K\n",
            "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
            "remote: Total 239 (delta 119), reused 186 (delta 94), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (239/239), 61.56 KiB | 12.31 MiB/s, done.\n",
            "Resolving deltas: 100% (119/119), done.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p nmt\n",
        "%cd nmt\n",
        "!git clone https://github.com/ymoslem/MT-Preparation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8g6so3PXh8Kc",
        "outputId": "3a009601-368d-42c5-da7e-0b367e3355d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 2)) (1.5.3)\n",
            "Collecting sentencepiece (from -r MT-Preparation/requirements.txt (line 3))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->-r MT-Preparation/requirements.txt (line 2)) (1.16.0)\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "# Install the requirements\n",
        "!pip3 install -r MT-Preparation/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "US7HA0aciCeP",
        "outputId": "de3c39cc-eab7-4e7f-bbc1-b7859a9a6ea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd  ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_mdE7WYWjN1_",
        "outputId": "6c1b59f4-282c-4be2-9dd0-8a153fc29dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.yaml  drive  models  nmt  run  sample_data  train.log\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j5UXPziljgVp",
        "outputId": "283476ac-ce5d-4ad1-aaba-601520089737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done desubwording! Output: /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/Tsonga_Ukuxhumana.Translated.desubword\n"
          ]
        }
      ],
      "source": [
        "!python3 nmt/MT-Preparation/subwording/3-desubword.py /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/target.model /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/Tsonga_Ukuxhumana.Translated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zHTXW0P3kw0K",
        "outputId": "187b30a4-4707-48d1-a69b-fff4fd9b1908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done desubwording! Output: /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/English_Tsonga_Tsonga_test.txt.subword.desubword\n"
          ]
        }
      ],
      "source": [
        "!python3 nmt/MT-Preparation/subwording/3-desubword.py /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/target.model /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/English_Tsonga_Tsonga_test.txt.subword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5zcDtaHujn3P",
        "outputId": "5b6c8af7-8dca-41f9-f5bd-1db8e9d9d079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-07-30 22:15:58--  https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 957 [text/plain]\n",
            "Saving to: ‘compute-bleu.py’\n",
            "\n",
            "compute-bleu.py     100%[===================>]     957  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-30 22:15:58 (49.5 MB/s) - ‘compute-bleu.py’ saved [957/957]\n",
            "\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.7.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.22.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Download the BLEU script\n",
        "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
        "!pip3 install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Cys-wvvakOmQ",
        "outputId": "92322075-50be-4911-c195-aee729457ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reference 1st sentence: i nhlokomhaka ya masocha ya lava ha riki vana .\n",
            "MTed 1st sentence: eka nhlokomhaka ya masocha ya n'wana .\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "BLEU:  62.432200444452036\n"
          ]
        }
      ],
      "source": [
        "!python3 compute-bleu.py /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/English_Tsonga_Tsonga_test.txt.subword.desubword /content/drive/MyDrive/LanguageTranslationData_New/EngTsonga_Ukuxhumna/Tsonga_Ukuxhumana.Translated.desubword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0C1zRojARqc3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2p9HsIvpmIve"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}